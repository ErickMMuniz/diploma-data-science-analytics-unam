---
title: "K means"
author: "Luis Daniel López Muñoz"
output: pdf_document
---


```{r}
library(tidyverse)
library(janitor)
library(skimr)
library(glue)
library(readr)
library(recipes)
library(cluster)
library(factoextra)
library(scatterplot3d)
library(plotly)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center"
)
set.seed(123)
```

Este documento realiza **aprendizaje no supervisado (K-means)** sobre el conjunto `habits.csv` (1,000 estudiantes) para identificar **perfiles de hábitos** y relacionarlos con el **exam score**. El flujo será: *exploración → preprocesamiento → selección de k → K-means → interpretación*.


Leemos los datos

```{r , include=FALSE}
ruta_base<-"habits.csv"
```

```{r}
Datos<-read.csv(ruta_base)
```

Comenzaremos por ver si nuestros datos contienen variables con valor "NA".

```{r valores faltantes}
# Conteo de NA por columna
na_profile <- Datos %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "na_count") %>%
  arrange(desc(na_count))

na_profile %>% filter(na_count > 0)
```

De lo cual, obtenemos que no tenemos datos faltantes.

Ahora veremos que tipo de datos constituyen cada columna:

```{r tipos de datos}
type_profile <- Datos %>%
  summarise(across(everything(), ~class(.x)[1])) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "class")

type_profile
```


## Procesamiento de Datos

Aunque el conjunto de datos incluye variables categóricas (por ejemplo, *género*, *tipo de empleo parcial*, *calidad de dieta* o *participación extracurricular*), estas no se incluyen en el proceso de entrenamiento del algoritmo **K-means**.  

El motivo es que **K-means se basa en distancias euclidianas**, las cuales requieren variables numéricas y continuas. Si las variables categóricas se transformaran en indicadores binarios (*one-hot encoding*), podrían distorsionar los resultados, ya que la presencia o ausencia de una categoría tendría el mismo peso que cambios en variables numéricas como las horas de estudio o de sueño.  

En lugar de forzar su inclusión, se opta por un enfoque más sólido:  
- **Entrenar el modelo solo con variables numéricas**, que son más apropiadas para la métrica de distancia de K-means.  
- **Utilizar las variables categóricas posteriormente para interpretar los clusters**, analizando cómo se distribuyen estas características en cada grupo identificado.  

Este procedimiento permite generar clusters más robustos y, al mismo tiempo, enriquecer su interpretación con la información cualitativa del conjunto de datos.

```{r preprocesamiento}
# Selección de variables numéricas para clustering
Datos_num <- Datos %>%
  select(age, study_hours_per_day, social_media_hours, netflix_hours,
         attendance_percentage, sleep_hours, exercise_frequency,
         mental_health_rating, exam_score)

# Normalización (media = 0, desviación estándar = 1)
X_scaled <- scale(Datos_num)

# Resumen estadístico de las variables ya escaladas
summary(as.data.frame(X_scaled))
```

## Selección del número de clusters

La elección adecuada de **k**, es decir, el número de clusters, es un paso fundamental en el algoritmo **K-means**.  
Uno de los métodos más utilizados es el **método del codo (*Elbow method*)**, que evalúa cómo disminuye la **suma de distancias al cuadrado dentro de los clusters (WSS, *Within-Cluster Sum of Squares*)** a medida que aumenta el número de clusters.

La lógica es la siguiente:

- A medida que incrementamos **k**, la WSS siempre disminuye porque los clusters se vuelven más pequeños y homogéneos.  
- Sin embargo, llega un punto en el que la reducción adicional de WSS es marginal.  
- Ese punto de inflexión se denomina **“codo”**, y es el valor sugerido para **k**.


```{r elbow, fig.width=6, fig.height=4}
fviz_nbclust(X_scaled, kmeans, method = "wss") +
  labs(title = "Método del codo (Elbow method)",
       x = "Número de clusters (k)")
```


En el gráfico del método del codo, la disminución de la **WSS** no presenta un punto de inflexión claro.  
La curva desciende de manera progresiva desde \(k = 1\) hasta \(k = 10\), sin un cambio de pendiente lo suficientemente marcado como para señalar un único valor de \(k\).  

Cuando el “codo” no es evidente, el criterio del método se vuelve ambiguo y es recomendable apoyarse en otros enfoques, como el **índice de Silhouette**, que permite evaluar directamente la calidad de la partición generada para cada número de clusters.


### 3.1 Índice de Silhouette

El **índice de Silhouette** evalúa simultáneamente la **compacidad interna** de los clusters y su **separación** respecto a los demás grupos.  
Para cada observación, compara la distancia media a los puntos de su propio cluster con la distancia media al cluster vecino más cercano.  
El valor promedio por partición toma valores en \([-1, 1]\):  
- cercano a **1** → clusters bien separados y compactos;  
- cerca de **0** → fronteras difusas entre clusters;  
- **negativo** → asignaciones potencialmente incorrectas.

Seleccionaremos el \(k\) cuyo **promedio de Silhouette** sea **máximo**.

```{r silhouette, fig.width=6, fig.height=4}

# Gráfico automático del promedio de Silhouette por k
fviz_nbclust(X_scaled, kmeans, method = "silhouette") +
  labs(title = "Selección de k mediante índice de Silhouette",
       x = "Número de clusters (k)",
       y = "Silhouette promedio")
```

El gráfico del índice de Silhouette muestra que el valor máximo se alcanza en **k = 2**, con un promedio cercano a 0.125.  
A partir de **k = 3**, los valores disminuyen y se estabilizan en torno a 0.09–0.10, lo cual indica que dividir en más grupos no mejora de manera significativa la separación ni la compacidad de los clusters.  

Si bien los valores absolutos del índice no son elevados (lo cual refleja que los clusters no están fuertemente diferenciados), el criterio de Silhouette sugiere que la mejor partición de los datos se obtiene con **dos clusters**.


## Ejecución del algoritmo K-means

Con base en el análisis previo, el número de clusters seleccionado es **k = 2**, ya que fue el valor óptimo de acuerdo con el índice de Silhouette.  

En esta sección se ejecuta el algoritmo **K-means** sobre la matriz de variables normalizadas y se generan las asignaciones de cluster para cada estudiante.  
Posteriormente, se presentan dos salidas clave:
1. La **visualización en dos dimensiones** utilizando Análisis de Componentes Principales (PCA).  
2. La **tabla de centroides** (valores medios de cada variable en cada cluster), reescalados a su escala original para facilitar la interpretación.

```{r kmeans-exec, fig.width=6, fig.height=4}

# Entrenamiento de K-means con k = 2
set.seed(123)
km2 <- kmeans(X_scaled, centers = 2, nstart = 50, iter.max = 100)

# Agregar la asignación de clusters al data frame original
Datos_clustered <- Datos %>%
  mutate(cluster = factor(km2$cluster))

# Visualización en PCA 2D
fviz_cluster(km2, data = X_scaled,
             geom = "point", ellipse.type = "norm",
             main = "Visualización de clusters (k = 2)") 
```



La gráfica de dispersión obtenida mediante **PCA en dos dimensiones** muestra la distribución de los estudiantes en los dos clusters formados por K-means.  

Cada punto representa a un estudiante, y los colores distinguen los clusters. Las elipses alrededor de cada grupo ilustran la dispersión y solapamiento de las observaciones.

Se observa que:
- Los dos clusters presentan cierta separación, especialmente a lo largo de la primera componente principal (Dim1).  
- Existe una zona de **superposición en el centro**, lo cual indica que algunos estudiantes tienen características intermedias que no permiten una diferenciación tajante.  
- A pesar de ello, el modelo identifica **dos perfiles predominantes**, que se analizarán en mayor detalle a través de los centroides y la comparación de las variables numéricas y categóricas.

Para explorar mejor la separación entre grupos, se realiza una **reducción de dimensionalidad mediante PCA a tres componentes** y se grafica en 3D.  
- Si el documento se renderiza a **HTML**, se muestra una visualización **interactiva** con `plotly`.  
- Si se renderiza a **PDF**, se muestra una **versión estática** con `scatterplot3d`.

```{r pca-3d, fig.width=7, fig.height=5}
# PCA a 3 componentes sobre los datos escalados
pca3 <- prcomp(X_scaled, center = FALSE, scale. = FALSE)
pc_df <- as.data.frame(pca3$x[, 1:3])
names(pc_df) <- c("PC1", "PC2", "PC3")
pc_df$cluster <- factor(km2$cluster)

# Etiquetas con varianza explicada para los ejes
var_exp <- (pca3$sdev^2) / sum(pca3$sdev^2)
xl <- paste0("PC1 (", round(100 * var_exp[1], 1), "%)")
yl <- paste0("PC2 (", round(100 * var_exp[2], 1), "%)")
zl <- paste0("PC3 (", round(100 * var_exp[3], 1), "%)")

if (knitr::is_html_output()) {
  # --- Interactivo (HTML) ---
  # install.packages("plotly") si hace falta
  plot_ly(
    pc_df,
    x = ~PC1, y = ~PC2, z = ~PC3,
    color = ~cluster, symbol = ~cluster,
    symbols = c("circle", "triangle-up"),
    type = "scatter3d", mode = "markers",
    marker = list(size = 3)
  ) %>%
    layout(
      title = "Clusters en 3D (PCA)",
      scene = list(
        xaxis = list(title = xl),
        yaxis = list(title = yl),
        zaxis = list(title = zl)
      ),
      legend = list(title = list(text = "Cluster"))
    )
} else {
  # --- Estático (PDF) ---
  # install.packages("scatterplot3d") si hace falta
  library(scatterplot3d)
  cols <- c("#E64B35", "#00A087")  # colores distintos por cluster
  pchs <- c(16, 17)
  col_vec <- cols[as.integer(pc_df$cluster)]
  pch_vec <- pchs[as.integer(pc_df$cluster)]

  scatterplot3d(pc_df$PC1, pc_df$PC2, pc_df$PC3,
                color = col_vec, pch = pch_vec,
                main = "Clusters en 3D (PCA)",
                xlab = xl, ylab = yl, zlab = zl,
                grid = TRUE, box = FALSE)
  legend("topright", legend = levels(pc_df$cluster),
         col = cols, pch = pchs, bty = "n", cex = 0.9, title = "Cluster")
}
```


En la gráfica se observa lo siguiente:
- Los dos clusters se distinguen principalmente a lo largo de la **primera componente (PC1)**.  
- Existe un grado de **solapamiento** entre los grupos, consistente con el valor moderado del índice de Silhouette.  
- Aun con esta superposición, se aprecian **zonas de concentración diferenciadas** que reflejan dos perfiles predominantes de estudiantes.

## Interpretación de los clusters

El objetivo de esta sección es **caracterizar los clusters** formados por **K-means**. Para ello:

- Se comparan los **valores medios** de las **variables numéricas** en cada cluster.
- Se analizan las **distribuciones de las variables categóricas** por cluster, con el fin de **enriquecer la interpretación** de los perfiles.

### Comparación de variables numéricas.

```{r comparacion numéricos}

# Promedios por cluster
numeric_means <- Datos_clustered %>%
  group_by(cluster) %>%
  summarise(across(c(age, study_hours_per_day, social_media_hours, netflix_hours,
                     attendance_percentage, sleep_hours, exercise_frequency,
                     mental_health_rating, exam_score),
                   mean, na.rm = TRUE)) %>%
  ungroup()

numeric_means
```

- **Cluster 1**:  
  Se caracteriza por **menor dedicación académica** (≈ 2.6 horas de estudio al día), acompañado de **mayor tiempo en redes sociales y Netflix**.  
  Presenta una **menor asistencia a clases (83%)**, menos ejercicio físico y **calificaciones más bajas en el examen final (≈ 55.6 puntos)**.  
  Además, sus estudiantes reportan un **estado de salud mental más bajo**.

- **Cluster 2**:  
  Agrupa a los estudiantes con **hábitos más disciplinados**: en promedio estudian más de 4 horas al día, duermen y hacen ejercicio con mayor regularidad, y dedican menos tiempo a redes sociales y Netflix.  
  También presentan **mayor asistencia (85%)** y mejores indicadores de **salud mental**, lo que se refleja en un **mejor rendimiento académico (≈ 82.5 puntos)**.


Posteriormente, vemos la distribución de las variables categóricas que conforman nuestros datos
  
```{r compaación categóricas}

categorical_summary <- Datos_clustered %>%
  group_by(cluster) %>%
  summarise(
    gender_male_pct = mean(gender == "Male") * 100,
    part_time_job_yes_pct = mean(part_time_job == "Yes") * 100,
    diet_good_pct = mean(diet_quality == "Good") * 100,
    extracurricular_yes_pct = mean(extracurricular_participation == "Yes") * 100,
    internet_good_pct = mean(internet_quality == "Good") * 100,
    .groups = "drop"
  )

categorical_summary
```

Las diferencias entre clusters en las variables categóricas resultan menos marcadas que en las variables numéricas, aunque permiten observar algunos matices:

- **Género:** la proporción de hombres es muy similar en ambos grupos (≈48% en Cluster 1 vs 47% en Cluster 2).  
- **Trabajo a tiempo parcial:** ligeramente más frecuente en el Cluster 1 (23%) que en el Cluster 2 (20%).  
- **Calidad de dieta:** algo mejor en el Cluster 2 (39%) respecto al Cluster 1 (37%).  
- **Participación extracurricular:** prácticamente idéntica en ambos grupos (≈32%).  
- **Acceso a internet:** mayor en el Cluster 1 (47%) en comparación con el Cluster 2 (42%).  

En conjunto, se confirma que las **principales diferencias entre clusters están en los hábitos numéricos** (estudio, sueño, ocio, ejercicio y rendimiento académico), mientras que las variables categóricas aportan información complementaria pero con variaciones más leves.


